<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Next Word Recommeder System</title>
    <link rel="stylesheet" href="../assets/css/globalblog.css">
    <!-- Load Prism CSS files with defer -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

    <style>
        html {
            scroll-behavior: smooth;
            scroll-padding-top: 60px;
            /* Adjust based on your navbar height */
        }

        html,
        body {
            overflow-x: hidden;
            width: 100%;
        }

        /* General styling for the h1 tag */
        h1 {
            font-size: 3em;
            font-weight: bold;
            text-align: center;
            padding: 20px;
            margin: 10px 0;
            color: currentColor;
            border-radius: 20px;
            /* This makes the text color adapt to the current text color */
            transition: color 0.3s ease;
            /* Smooth transition for color change */
            /* Smooth transition for color change */
        }

        /* Light mode styles */
        @media (prefers-color-scheme: light) {
            h1 {
                color: #333;
                /* Dark color for text in light mode */
                background-color: #f9f9f9;
                /* Light background for light mode */
            }
        }

        /* Dark mode styles */
        @media (prefers-color-scheme: dark) {
            h1 {
                color: #f0f0f0;
                /* Light color for text in dark mode */
                background-color: #333;
                /* Dark background for dark mode */
            }
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.5em;
            }
        }

        p {
            text-align: justify;
        }

        p a {
            color: #007bff;
        }

        p a:hover {
            color: #33a004;
        }

        /* Styles for the notebook */
        .notebook {
            width: 100%;
            margin: 0 auto;
            padding: 10px;
            /* Minimal padding */
            box-sizing: border-box;
        }

        /* Cell style inside the notebook */
        .cell {
            background-color: rgba(0, 0, 0, 0.05);
            border-radius: 5px;
            padding: 10px;
            /* Minimal padding */
            width: 100%;
            color: currentColor;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            /* Subtle shadow */
            margin: 5px 0;
            /* Minimal margin */
        }



        /* Code block styling (pre and code) */
        pre {
            padding: 5px;
            /* Minimal padding */
            border-radius: 4px;
            /* Smaller radius for a more compact look */
            margin: 5px 0;
            /* Minimal margin */
            font-family: Cambria, Cochin, Georgia, Times, 'Times New Roman', serif;
            font-size: 15px;
            /* Slightly smaller font for mobile */
            line-height: 1.3;
            /* Tight line height for better space utilization */
            white-space: nowrap;
            /* Prevent wrapping of long lines of code */
            background-color: rgba(0, 0, 0, 0.05);
            color: #212121;
            border: 1px solid #ddd;
            overflow-x: auto;
            /* Horizontal scrolling */
            overflow-y: hidden;
            /* Disable vertical scroll if not needed */
        }

        pre code {
            font-weight: bolder;
        }

        pre table {
            border-collapse: collapse;
            width: 50%;
            font-family: inherit;
            font-size: inherit;
            text-align: left;
            margin: 20px 0;
            border: 1px solid #ddd;
        }

        pre table th,
        td {
            padding: 5px;
            border-right: 1px solid #ddd;
        }

        pre table th {
            border-bottom: 2px solid #dee2e6;
        }

        pre table tr {
            border-bottom: 1px solid #ddd;
        }

        pre table tr:last-child {
            border-bottom: none;
        }

        /* For dark mode */
        [data-theme="dark"] pre {
            background-color: rgba(0, 0, 0, 0.4);
            color: #f5f5f5;
            border: 1px solid #444;
        }

        /* Mobile view adjustments */
        @media (max-width: 600px) {
            .notebook {
                padding: 5px;
                /* Minimal padding for mobile */
            }

            .cell {
                padding: 8px;
                /* Slightly reduced padding for mobile */
            }

            pre {
                padding: 3px;
                /* Reduced padding for mobile */
                font-size: 12px;
                /* Smaller font for better fitting */
                margin: 3px 0;
                /* Minimal margin for mobile */
                max-height: none;
                /* No max-height to allow scrolling */
                overflow-x: auto;
                /* Enable horizontal scrolling */
                white-space: nowrap;
                /* Prevent line wrapping */
            }
        }

        /* Default styles for light mode */
        section h3 {
            font-size: 1.5em;
            font-weight: 600;
            color: #000000;
            margin: 10px 0;
            padding: 5px 0;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            border-bottom: 2px solid #e0e0e0;
            transition: color 0.3s ease, transform 0.3s ease, border-bottom-color 0.3s ease;
        }

        /* Hover effect */
        section h3:hover {
            color: #007bff;
            transform: translateX(5px);
            border-bottom-color: #007bff;
        }

        [data-theme="dark"] section h3 {
            color: #dbcaca;
            /* Lighter color for dark mode */
            border-bottom: 2px solid #444;
            /* Darker border for dark mode */
        }

        [data-theme="dark"] section h3:hover {
            color: #ff6347;
            /* Change color to a warm color (e.g., tomato) for dark mode hover */
            transform: translateX(5px);
            /* Slight movement to the right on hover */
            border-bottom-color: #ff6347;
            /* Change border color to tomato on hover */
        }

        footer {
            background-color: #F2F0EF;
            color: #000000;
            text-align: center;
            padding: 20px 0;
            font-family: Arial, sans-serif;
        }

        [data-theme="dark"] footer {
            background-color: #1f2937;
            color: #f5f5f5;
        }

        .social-media h3 {
            font-size: 20px;
            margin-bottom: 10px;
            color: #000000;
        }

        [data-theme="dark"] .social-media h3 {
            color: #f5f5f5;
        }

        .icons {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-top: 10px;
        }

        .icons a {
            color: #000000;
            font-size: 24px;
            transition: transform 0.3s ease, color 0.3s ease;
        }

        [data-theme="dark"] .icons a {
            color: #f5f5f5;
        }

        .icons a:hover {
            transform: scale(1.2);
            color: #ffcc00;
            /* Change color on hover */
        }

        /* Individual colors for branding */
        .icons a:nth-child(1):hover {
            color: #E1306C;
        }

        /* Instagram */
        .icons a:nth-child(2):hover {
            color: #0077B5;
        }

        /* LinkedIn */
        .icons a:nth-child(3):hover {
            color: grey;
        }

        /* GitHub */
        .icons a:nth-child(4):hover {
            color: #20BEFF;
        }

        /* Kaggle */
        .icons a:nth-child(5):hover {
            color: #1DA1F2;
        }

        /* Twitter/X */
        .icons a:nth-child(6):hover {
            color: #34A853;
        }

        /* email */
        @media (max-width: 600px) {
            .icons {
                gap: 10px;
            }

            .icons a {
                font-size: 20px;
            }
        }
    </style>
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3MEXW2XNBM"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-3MEXW2XNBM');
</script>

<body>
    <div class="app">
        <!-- Mobile Nav Overlay -->
        <div id="navOverlay" class="nav-overlay hidden"></div>

        <!-- Navigation -->
        <nav id="sidebar" class="sidebar">
            <div class="nav-content">
                <button id="closeNav" class="close-nav">&times;</button>
                <h2 class="nav-title">Table of Contents</h2>
                <div class="chapters">
                    <div class="chapter">
                        <h2><a href="#chapter1">Introduction</a></h2>
                        <ul>
                            <li><a href="#aboutTheProject">Abstract</a></li>
                            <li><a href="#introductionToNextWordPrediction">Introduction to Next Word Prediction</a>
                            </li>
                            <li><a href="#problemStatement">Problem Statement</a></li>
                            <li><a href="#objectiveOfTheProject">Objective of the Project</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="#chapter2">Dataset and Data Preprocessing</a></h2>
                        <ul>
                            <li><a href="#datasetDescription">Dataset Description</a></li>
                            <li><a href="#dataCleaningAndPreprocessing">Data Cleaning and Preprocessing</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="#chapter3">N-gram Based Approach</a></h2>
                        <ul>
                            <li><a href="#understandingNgram">Understanding N-gram Model</a></li>
                            <li><a href="#implementation">Implementation</a></li>
                            <li><a href="#samplePredictions">Sample Predictions</a></li>
                        </ul>
                    </div>


                    <div class="chapter">
                        <h2><a href="#chapter4">Results and Analysis</a></h2>
                        <ul>
                            <li><a href="#Results">Results</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="#chapter5">Conclusion</a></h2>
                        <ul>
                            <li><a href="#futureImprovements">Future Improvements</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="#chapter6">References and Resources</a></h2>

                    </div>


                </div>
            </div>
        </nav>


        <!-- Main Content -->
        <div class="main-content">
            <!-- Header -->
            <header class="header">
                <button id="menuButton" class="menu-button">
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <line x1="4" x2="20" y1="12" y2="12" />
                        <line x1="4" x2="20" y1="6" y2="6" />
                        <line x1="4" x2="20" y1="18" y2="18" />
                    </svg>
                </button>

                <button id="themeToggle" class="theme-toggle">
                    <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="4" />
                        <path d="M12 2v2" />
                        <path d="M12 20v2" />
                        <path d="m4.93 4.93 1.41 1.41" />
                        <path d="m17.66 17.66 1.41 1.41" />
                        <path d="M2 12h2" />
                        <path d="M20 12h2" />
                        <path d="m6.34 17.66-1.41 1.41" />
                        <path d="m19.07 4.93-1.41 1.41" />
                    </svg>
                    <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z" />
                    </svg>
                </button>

                <div class="right-buttons">
                    <a href="https://shivrajanand.github.io" class="btn">PORTFOLIO</a>
                </div>
            </header>

            <!-- Content -->
            <main class="content">
                <article class="article">
                    <h1>Next Word Recommender System using Probabilistic Models</h1>

                    <section id="chapter1">
                        <h2>Introduction</h2>
                        <h3 id="aboutTheProject">Abstract</h3>
                        <p>
                            Next Word Prediction is an elementary problem in Natural Language Processing (NLP) that is
                            used in search engines, text editors, and AI writing assistants. In this blog, a
                            probabilistic N-gram strategy to predict the next word of a sentence by statistical language
                            modeling is discussed. The N-gram model relies on word frequency probabilities rather than
                            deep learning-based approaches to produce predictions with efficient computations.
                        </p>
                        <p>
                            This project was done as a part of my 7th-semester B.Tech internship at Internshala, where I
                            implemented a Next Word Prediction system using data preprocessing, N-gram probability
                            calculations, and text-based evaluations. The blog includes the significance of Next Word
                            Prediction, processing the dataset, implementation details, and example predictions.
                            Further, we also present the drawbacks of N-gram models, including their reliance on
                            training data and absence of contextual knowledge, as well as some possible enhancements
                            with smoothing methods and hybrid models.
                        </p>
                        <p>
                            This blog is a helpful guide for anyone wanting to create a simple yet useful Next Word
                            Prediction system based on traditional probabilistic techniques.
                        </p>
                        <p>
                            You can access the entire project <a
                                href="https://www.kaggle.com/code/shivrajanandai/next-word-recommender-system"><strong>from
                                    this kaggle link</strong></a>. Also go through my other works from <a
                                href="https://shivrajanand.github.io/pages/blog.html"><strong>here</strong></a>.
                        </p>


                        <h3 id="introductionToNextWordPrediction">Introduction to Next Word Prediction</h3>
                        <p>As digital communication has expanded at a tremendous pace, Next Word Prediction is now a
                            vital feature in contemporary text-based programs. You type on a smartphone keypad, you
                            search using an engine, or you write emails, predictive text assists in making typing
                            faster, more accurate, and convenient for the user.</p>

                        <p>Next Word Prediction is one of the most basic problems of Natural Language Processing (NLP),
                            in which the task is to predict the next most likely word from the context provided. There
                            are multiple methods to address this problem, from rule-based models to powerful deep
                            learning methodologies. In our project, we work with a probabilistic N-gram approach, which
                            takes advantage of statistical patterns in texts to predict the next word at a low
                            computational cost.</p>

                        <p>The N-gram model is an efficient and easy technique for language modeling. It determines the
                            probability of a word in a sequence based on the words that precede it. The method is highly
                            used because of its low computational complexity and readability, making it an ideal
                            technique for scenarios where deep learning cannot be used.</p>
                        <p>
                            In this blog, we shall discuss:
                        <ul style="list-style-type: circle; margin-left: 20px;">
                            <li>The significance and usage of Next Word Prediction</li>
                            <li> How N-gram models function and their application</li>
                            <li>Example predictions and limitations</li>
                        </ul>
                        </p>
                        <h3 id="problemStatement">Problem Statement</h3>
                        <p><strong>Develop a next word recommendation system which should take in a seed text from the
                                user as input and predict the next relevant word.</strong></p>
                        <h3 id="objectiveOfTheProject">Objective of the Project</h3>
                        <p>The objective of this project is to create a Next Word Recommendation System that accepts a
                            user-provided text input and predicts the most appropriate next word. Rather than using
                            sophisticated machine learning models, this project emphasizes a probabilistic method based
                            on N-grams, which utilizes statistical language patterns to make predictions.</p>
                        <p>Through the construction of this system, the goal is to have a thorough, hands-on
                            comprehension of how next-word prediction operates at its core. This project enables us to
                            delve into language modeling, probability distributions, and text processing without having
                            to explicitly train with sophisticated ML algorithms. It is a stepping stone for
                            comprehending both classical NLP methods and their practical applications in predictive text
                            systems.</p>
                    </section>
                    <section id="chapter2">
                        <h2 id="chapter2">Dataset and Data Preprocessing</h2>

                        <h3 id="datasetDescription">Dataset Description</h3>
                        <p>The Taskmaster Conversational Dataset, released in 2019, is a 64,777 conversational dialogue
                            dataset and has been proposed for the training and testing of dialogue systems as well as
                            next-word prediction models to offer authentic, goal-driven real-world conversations
                            spanning various domains.</p>
                        <p>
                            Features:
                        <ul style="margin-left: 1vh;">
                            <li><strong>Size</strong>: 64,777 dialogues</li>
                            <li><strong>Type</strong>: Task-based, goal-oriented dialogues</li>
                            <li><strong>Domains included:</strong></li>
                            <ul style="margin-left: 1.5vh; list-style-type:num">
                                <li>Ordering Pizza 🍕</li>
                                <li>Setting Up a Ride Service 🚖</li>
                                <li>Making Restaurant Reservations 🍽️</li>
                                <li>Creating Auto Repair Appointments🚗</li>
                                <li>Ordering Coffee Drinks ☕</li>
                                <li>Ordering Movie Tickets 🎬</li>
                            </ul>
                        </ul>
                        </p>
                        <p>Each of the conversations in the dataset is an actual interaction between a user and an
                            automated system or a human helper and thus is perfect for learning natural language
                            understanding, conversational AI, and predictive text models.</p>
                        <p>For this project, the data is utilized to train a probabilistic Next Word Prediction system
                            on an N-gram model that learns patterns in language from such task-oriented dialogues. The
                            structured form of the data allows for insight into how individuals convey information in
                            service-based interactions and enables more informed and context-savvy text predictions.</p>
                        <h3 id="dataCleaningAndPreprocessing">Data Cleaning and Preprocessing</h3>
                        <h4>Loading the dataset</h4>
                        <div class="notebook">
                            <div class="cell">
                                <pre><code>with open("/kaggle/input/task-master-conversational-dataset/dialogs_dataset", "rb") as f: <br>&emsp13;&emsp13; dialogs = pickle.load(f)</code></pre>
                            </div>
                            <div class="cell">
                                <pre><code>random.sample(dialogs_clean, 10)</code></pre>
                                <pre>
                                    ["I'm in Orlando Florida", <br>
                                    "OK, I'll do that",<br>
                                    'No that is all',<br>
                                    'You covered it all! Thanks!',<br>
                                    "I'll take that please",<br>
                                    ' Can you add a cheese pizza to that, too, actually? No toppings',<br>
                                    'No that is all for now',<br>
                                    ' I need you to schedule an appointment for my vehicle?',<br>
                                    "I would like to order a pizza from Papa John's",<br>
                                    "Okay I'll be there thank you"]</pre>
                            </div>
                        </div>
                        <h4>cleaning the dataset</h4>
                        <p>Everything except alphabets and white spaces is removed</p>
                        <div class="notebook">
                            <div class="cell">
                                <pre><code>
                                    dialogs_clean = []<br>
                                    for i in dialogs: <br>
                                        &emsp13;&emsp13;i = re.sub("[^a-zA-Z' ]", "", i)<br>
                                        &emsp13;&emsp13;i = i.lower()<br>
                                        &emsp13;&emsp13;dialogs_clean.append(i)
                                </code></pre>
                            </div>

                            <div class="cell">
                                <pre><code>random.sample(dialogs_clean, 10)</code></pre>
                                <pre>
                                    [' how much are the ticket prices',<br>
                                     'can i get a pepperoni and pineapple pizza',<br>
                                     " and the driver's name",<br>
                                     'large  please',<br>
                                     'just four of us',<br>
                                     "that's perfect thank you",<br>
                                     'do they have buffet',<br>
                                     "yes that's all i need",<br>
                                     'hello there i would like to order an ultimate pepperoni ',<br>
                                     'i need new front tires but my rear tires are fine']</pre>
                            </div>
                            <h4>Creating the vocabulary</h4>
                            <p><strong>Vocabulary</strong> is the set of all unique words in a corpus(dataset).
                                Extracting this is necessary to calculate probabilities of n-gram I will obtain in
                                upcoming section.</p>

                            <div class="cell">
                                <pre><code>
                                    all_words = " ".join(dialogs_clean).split() <br>
                                    words_dict = {} <br>
                                    for word in all_words: <br>
                                        &emsp13;&emsp13;for word in all_words: <br>
                                            &emsp13;&emsp13;&emsp13;&emsp13;if word not in words_dict: <br>
                                        &emsp13;&emsp13;else: <br>
                                            &emsp13;&emsp13;&emsp13;&emsp13;words_dict[word] = 1

                                            &emsp13;&emsp13;for word in all_words: <br>
                                            &emsp13;&emsp13;&emsp13;&emsp13;else: <br>
                                        &emsp13;&emsp13;else: <br>
                                            &emsp13;&emsp13;&emsp13;&emsp13;words_dict[word] += 1
                                </code></pre>
                            </div>

                            <h4>Creating Word Count Dataframe</h4>
                            <p>Word count dataframe is a 2D table with two colums, column 1 consists of all the unique
                                words (vocabulary) and column 2 consists of number of occurences of the unique word in
                                the dataset.</p>

                            <div class="cell">
                                <pre><code>
                                    words_df = pd.DataFrame({'word':list(words_dict.keys()), 'count':list(words_dict.values())}) <br>
                                    words_df = words_df.sort_values(by = ['count']) <br>
                                    words_df.reset_index(drop=True, inplace=True)
                                </code></pre>
                            </div>

                            <div class="notebook">
                                <div class="cell">
                                    <pre><code>words_df.head()</code></pre>
                                    <pre>
                                        <table>
                                            <tr>
                                                <th></th>
                                                <th>word</th>
                                                <th>count</th>
                                            </tr>
                                            <tr>
                                                <td>0</td>
                                                <td>uppermiddle</td>
                                                <td>1</td>
                                            </tr>
                                            <tr>
                                                <td>1</td>
                                                <td>shoots</td>
                                                <td>1</td>
                                            </tr>
                                            <tr>
                                                <td>2</td>
                                                <td>geesh</td>
                                                <td>1</td>
                                            </tr>
                                            <tr>
                                                <td>3</td>
                                                <td>andrea</td>
                                                <td>1</td>
                                            </tr>
                                            <tr>
                                                <td>4</td>
                                                <td>precice</td>
                                                <td>1</td>
                                            </tr>
                                        </table>
                                    </pre>

                                </div>

                                <div class="cell">
                                    <pre><code>words_df.tail()</code></pre>
                                    <pre>
                                        <table>
                                            <tr>
                                                <th></th>
                                                <th>word</th>
                                                <th>count</th>
                                            </tr>
                                            <tr>
                                                <td>11142</td>
                                                <td>you</td>
                                                <td>11909</td>
                                            </tr>
                                            <tr>
                                                <td>11143</td>
                                                <td>a</td>
                                                <td>13380</td>
                                            </tr>
                                            <tr>
                                                <td>11144</td>
                                                <td>to</td>
                                                <td>14000</td>
                                            </tr>
                                            <tr>
                                                <td>11145</td>
                                                <td>the</td>
                                                <td>15406</td>
                                            </tr>
                                            <tr>
                                                <td>11146</td>
                                                <td>i</td>
                                                <td>19654</td>
                                            </tr>
                                        </table>
                                    </pre>
                                </div>
                            </div>


                    </section>

                    <section id="chapter3">
                        <h2 id="chapter3">N-gram Based Approach</h2>
                        <h3 id="understandingNgram">Understanding N-gram Model</h3>
                        <p>The N-gram model is a statistical language model applied for making predictions about what
                            the next object in a stream of text would be, depending on the (N−1) preceding ones. It's
                            among the most basic and well-established methods employed in Natural Language Processing
                            (NLP) applications such as predicting text, correcting spelling, and machine translation.
                        </p>

                        <p>An N-gram is a sequence of N items in a text or speech corpus. These can be characters,
                            syllables, or more often, words. Based on the value of N, the model has varying context:</p>

                        <ul style="margin-left: 20px;">
                            <li>Unigram (N=1): Takes into account each word separately.</li>
                            <li>Bigram (N=2): Takes into account the current word depending on the last one.</li>
                            <li>Trigram (N=3): Takes into account the current word depending on the two preceding ones.
                            </li><br>
                        </ul>
                        <p>
                            The key idea is that the probability of a word appearing depends only on the previous (N−1)
                            words, simplifying the complex dependencies in natural language.
                        </p>

                        <h4>Mathematical Formulation</h4>
                        <p>$$P(w_n|w_1, w_2, ..., w_{n-1}) \approx P(w_n | w_{n-(N-1)}, ..., w_{n-1}) $$</p>
                        <p>These probabilities are estimated from a training corps using frequency counts: </p>
                        <p>$$P(w_n|w_{n-2},w_{n-1}) = \frac{Count(w_{n-2},w_{n-1},w_n)}{Count(w_{n-2},w_{n-1})}$$</p>


                        <h3 id="implementation">Implementation</h3>

                        <h4>1. Creating Dataframe</h4>
                        <iframe
                            src="https://www.kaggle.com/embed/shivrajanandai/next-word-recommender-system?cellIds=18-30&kernelSessionId=226487950"
                            height="500" style="margin: 0 auto; width: 100%; max-width: 950px;" frameborder="0"
                            scrolling="auto" title="Next Word Recommender System"></iframe>


                        <h3 id="samplePredictions">Sample Predictions</h3>
                        <p>Let's see some sample predictions made by the N-gram model:</p>
                        <div class="notebook">
                            <div class="cell">
                                <pre><code>print(dict(model[('to', 'book')]))</code></pre>
                                <pre>{'a': 186, 'an': 107, 'at': 9, 'for': 5, 'me': 16, 'some': 13, 'your': 3, 'it': 9, 'the': 11, 'reservations': 6, 'this': 3, 'tickets': 6, 'two': 6, 'flight': 1, 'with': 2, 'reservation': 1, 'shared': 1, 'that': 3, 'movie': 4, 'there': 3, 'four': 1, 'any': 1, 'my': 2, 'through': 1, 'us': 1, 'and': 1, 'anything': 1, 'anymore': 1, "patty's": 1, 'later': 1, 'something': 1, 'uberxl': 1, 'dinner': 1}</pre>
                            </div>
                            <div class="cell">
                                <pre><code>dict(model["good", "to"])</code></pre>
                                <pre>{'know': 20,
                                    'me': 56,
                                    'go': 10,
                                    'hear': 6,
                                    'the': 1,
                                    'you': 2,
                                    'drive': 1,
                                    'watch': 1,
                                    'eat': 1,
                                    'pass': 1,
                                    'bring': 1}</pre>
                            </div>
                        </div>
                        <h4>Probabilistic Output</h4>
                        <div class="notebook">
                            <div class="cell">
                                <pre><code>
                                    unigram_dict = {}<br>
                                    <br>
                                    for i in tqdm(range(dataset.shape[0])):<br>
                                    &emsp;&emsp;for word in dataset['unigram'][i]:<br>
                                    &emsp;&emsp;&emsp;&emsp;if word[0] in unigram_dict:<br>
                                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;unigram_dict[word[0]] += 1<br>
                                    &emsp;&emsp;&emsp;&emsp;else:<br>
                                    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;unigram_dict[word[0]] = 1
                                </code></pre>
                            </div>
                            <div class="cell">
                                <pre><code>counts = Counter(unigram_dict)</code></pre>
                                <pre><code>total_count = len(unigram_dict)</code></pre>
                                <pre><code>
                                    for word in counts: <br>&emsp;&emsp;counts[word] /= float(total_count)
                                </code></pre>
                                <pre><code>
                                    for w1_w2 in model:<br>
&emsp;&emsp;total_count = float(sum(model[w1_w2].values()))<br>
&emsp;&emsp;for w3 in model[w1_w2]:<br>
&emsp;&emsp;&emsp;&emsp;model[w1_w2][w3] /= total_count

                                </code></pre>
                            </div>

                        </div>
                    </section>

                    <section id="chapter4">
                        <h3 id="Results">Results</h3>
                        <div class="notebook">
                            <div class="cell">
                                <pre><code>dict(model["to", "book"])</code></pre>
                                <pre>
                                    {   'a': 0.4547677261613692<br>
                                        'an': 0.2616136919315403<br>
                                        'at': 0.022004889975550123<br>
                                        'for': 0.012224938875305624<br>
                                        'me': 0.039119804400977995<br>
                                        'some': 0.03178484107579462<br>
                                        'your': 0.007334963325183374<br>
                                        'it': 0.022004889975550123<br>
                                        'the': 0.02689486552567237<br>
                                        'reservations': 0.014669926650366748<br>
                                        'this': 0.007334963325183374<br>
                                        'tickets': 0.014669926650366748<br>
                                        'two': 0.014669926650366748<br>
                                        'flight': 0.0024449877750611247<br>
                                        'with': 0.004889975550122249<br>
                                        'reservation': 0.0024449877750611247<br>
                                        'shared': 0.0024449877750611247<br>
                                        'that': 0.007334963325183374<br>
                                        'movie': 0.009779951100244499<br>
                                        'there': 0.007334963325183374<br>
                                        'four': 0.0024449877750611247<br>
                                        'any': 0.0024449877750611247<br>
                                        'my': 0.004889975550122249<br>
                                        'through': 0.0024449877750611247<br>
                                        'us': 0.0024449877750611247<br>
                                        'and': 0.0024449877750611247<br>
                                        'anything': 0.0024449877750611247<br>
                                        'anymore': 0.0024449877750611247<br>
                                        "patty's": 0.0024449877750611247<br>
                                        'later': 0.0024449877750611247<br>
                                        'something': 0.0024449877750611247<br>
                                        'uberxl': 0.0024449877750611247<br>
                                        'dinner': 0.0024449877750611247<br>}
                                </pre>
                            </div>
                        </div>

                        <section id="chapter5">
                            <h2 id="chapter6">Conclusion</h2>
                            <p> I learned, through this project, how to construct a Next Word Prediction system from the
                                N-gram language model. Breaking down text into unigrams, bigrams, and trigrams allowed
                                me to examine word patterns and apply them to predict the most likely next word based on
                                previous context. It was interesting to observe how basic statistical methods can be the
                                foundations for effective language utilities we utilize in our daily lives.</p>
                            <p>Although the N-gram method was simple and practical, I was also aware of its
                                weaknesses—particularly in addressing unseen word collations and contextual dependencies
                                of length greater than n. Even then, the model made sensible predictions in most
                                scenarios and provided insight into how vanilla NLP models operate. </p>
                            <p>This experience has laid a strong foundation for moving toward more advanced models in
                                the future. I’m excited to explore smoothing techniques, experiment with larger
                                datasets, and eventually dive into deep learning methods like RNNs and transformers to
                                build smarter, more context-aware prediction systems.</p>
                            <h3 id="futureImprovements">Future Improvements</h3>
                            <p>
                                Although the N-gram model was a good foundation for predicting next words, there are a
                                number of improvements that I can make on this project in the future. One key area for
                                improvement is dealing with unseen or low-frequency combinations of words. To achieve
                                this, I will look into smoothing methods such as Laplace or Kneser-Ney smoothing, which
                                will assist in providing probabilities for n-grams never before seen and enhance the
                                overall accuracy of the model.
                            </p>
                            <p>
                                Another enhancement would be to utilize a larger and more varied dataset. The current
                                dataset, while good for experimenting, is lacking in vocabulary and context. Enlarging
                                the dataset would make the model more generalizable and better at predicting across
                                topics.
                            </p>
                            <p>
                                I also want to experiment with higher-order n-grams, although I’m aware this comes with
                                increased computational costs and data sparsity issues. Still, it could potentially
                                capture richer context.
                            </p>
                            <p>
                                Eventually, I’d like to move beyond traditional N-gram models and explore neural
                                network-based language models such as RNNs, LSTMs, or transformers like BERT and GPT.
                                These models are much better at capturing long-range dependencies and semantic meaning,
                                which could significantly boost the performance and usefulness of the prediction system.
                            </p>
                            <p>Finally, incorporating a user interface where users can input and receive word
                                suggestions in real-time would make the project more interactive and useful.</p>
                        </section>
                        <section id="chapter6">
                            <h2 id="chapter6">References and Resources</h2>
                            <ol>
                                <li>Jurafsky, D., & Martin, J. H. (2020). <i>Speech and Language Processing</i> (3rd ed.
                                    draft). <a href="https://web.stanford.edu/~jurafsky/slp3/"
                                        target="_blank">https://web.stanford.edu/~jurafsky/slp3/</a></li>

                                <li>Wikipedia contributors. “N-gram.” <i>Wikipedia</i>, The Free Encyclopedia. <a
                                        href="https://en.wikipedia.org/wiki/N-gram"
                                        target="_blank">https://en.wikipedia.org/wiki/N-gram</a></li>

                                <li>Manning, C. D., Raghavan, P., & Schütze, H. (2008). <i>Introduction to Information
                                        Retrieval</i>. Cambridge University Press.</li>

                                <li>Python Documentation – <code>collections</code> module: <a
                                        href="https://docs.python.org/3/library/collections.html"
                                        target="_blank">https://docs.python.org/3/library/collections.html</a></li>

                                <li>Kaggle. Sample text datasets. <a href="https://www.kaggle.com/datasets"
                                        target="_blank">https://www.kaggle.com/datasets</a></li>

                                <li>Towards Data Science – N-gram Models in NLP: <a
                                        href="https://towardsdatascience.com/n-gram-models-and-their-applications-3f41d5a5aa5e"
                                        target="_blank">https://towardsdatascience.com/n-gram-models-and-their-applications-3f41d5a5aa5e</a>
                                </li>
                            </ol>
                        </section>
                </article>

            </main>

            <footer>
                <div class="social-media">
                    <h3>Follow Me</h3>
                    <div class="icons">
                        <a href="https://www.instagram.com/shivrajanandai/" target="_blank" rel="noopener noreferrer">
                            <i class="fab fa-instagram"></i>
                        </a>
                        <a href="https://www.linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=shivrajanand"
                            target="_blank" rel="noopener noreferrer">
                            <i class="fab fa-linkedin"></i>
                        </a>
                        <a href="https://www.github.com/shivrajanand" target="_blank" rel="noopener noreferrer">
                            <i class="fab fa-github"></i>
                        </a>
                        <a href="https://www.kaggle.com/shivrajanandai" target="_blank" rel="noopener noreferrer">
                            <i class="fab fa-kaggle"></i>
                        </a>
                        <a href="https://x.com/shivrajanand_ai" target="_blank" rel="noopener noreferrer">
                            <i class="fab fa-twitter"></i>
                        </a>

                        <a href="https://mailto::shivrajanand022002@gmail.com" target="_blank"
                            rel="noopener noreferrer">
                            <i class="fa fa-envelope"></i>
                        </a>

                    </div>
                </div>
            </footer>
        </div>
    </div>

    <!-- Load scripts with defer -->
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script defer src="../assets/js/globalblog.js"></script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>


    <!-- Fallback for Prism if CDN fails -->
    <script>
        window.addEventListener('load', function () {
            if (typeof Prism === 'undefined') {
                console.error('Prism failed to load');
                // Add a class to show code without highlighting
                document.querySelectorAll('pre code').forEach(block => {
                    block.classList.add('no-highlight');
                });
            }
        });
    </script>


</body>

</html>