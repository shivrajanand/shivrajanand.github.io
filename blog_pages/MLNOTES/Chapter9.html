<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter9: Ensemble Modelling</title>
    <link rel="stylesheet" href="../../assets/css/globalblog.css">
    <!-- Load Prism CSS files with defer -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <!-- Add error handling for script loading -->
    <script>
        window.addEventListener('error', function (e) {
            if (e.target.tagName === 'SCRIPT') {
                console.error('Script loading failed:', e.target.src);
                alert('Failed to load script: ' + e.target.src);
            }
        }, true);
    </script>

    <style>
        html {
            scroll-behavior: smooth;
            scroll-padding-top: 60px;
            /* Adjust based on your navbar height */
        }

        html,
        body {
            overflow-x: hidden;
            width: 100%;
        }


        #image {
            width: 100%;
            /* Or a specific width */
            height: auto;
            /* Or any height you prefer */
            overflow: hidden;
            /* Ensures no overflow */
            display: flex;
            flex-direction: column;
            /* Stack children (image and caption) vertically */
            justify-content: center;
            align-items: center;
            border: 1px solid black;
            /* Border around the container */
        }

        [data-theme="dark"] #image {
            border: 1px solid white;
            /* Dark mode border color */
        }

        #image figure {
            width: 100%;
            /* Ensure the figure takes up the full width */
            margin: 0;
            text-align: center;
            /* Center align the caption */
        }

        #image img {
            width: 100%;
            /* Ensures the image takes the full width of the container */
            height: auto;
            /* Maintains the aspect ratio */
            object-fit: contain;
            /* Ensure the image fits within the div */
        }

        #image figcaption {
            font-size: 1em;
            /* Caption text size */
            margin-top: 10px;
            /* Space between the image and caption */
            color: #333;
            /* Text color */
            font-style: italic;
            /* Italicize caption text */
            text-align: center;
            border-top: 2px solid #ccc;
            /* Border between image and caption */
            padding-top: 5px;
            /* Space between image and border */
            padding-bottom: 15px;
            /* Space below the caption for a clean look */
            width: 100%;
            /* Ensures the caption takes up the full width */
        }


        [data-theme="dark"] #image figcaption {
            color: white;
            border: white 1px solid;
        }



        p {
            text-align: justify;
        }

        /* Unordered List Style */
        ul {
            list-style-type: none;
        }

        ul .li {
            margin-bottom: 10px;
        }

        [data-theme="dark"] li a {
            color: white;
        }


        @media (max-width: 768px) {
            #image {
                width: 100%;
                /* or a specific width */
                /* or any height you prefer */
                overflow: hidden;
                /* Ensures no overflow */
            }

            #image img {
                width: 100%;
                object-fit: contain;
                /* Ensure the image fits within the div while maintaining its aspect ratio */
            }

            img {
                width: 100%;
                height: auto;
            }
        }

        .notebook {
            width: 100%;
            margin: auto;
        }

        .cell {
            background-color: rgba(0, 0, 0, 0.3);
            border-radius: 10px;
            padding: 20px;
            width: 100%;
            color: currentColor;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            margin: 10px 0;
        }

        .input,
        .output {
            padding: 10px;
            border-radius: 5px;
            text-align: left;
        }

        pre {
            overflow-x: auto;
            text-align: left;
        }

        @media (max-width: 600px) {
            .notebook {
                width: 100%;
                padding: 10px;
            }
        }

        .graph-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .graph-item {
            display: flex;
            align-items: center;
            gap: 15px;
        }

        .graph-img {
            flex: 1;
            text-align: center;
        }

        .graph-img img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }

        .graph-desc {
            flex: 1;
            padding: 10px;
            border-radius: 10px;
            background-color: var(--card-bg);
            color: var(--text-color, #333);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }

        .graph-desc h3 {
            margin-bottom: 5px;
        }

        .graph-desc p {
            margin: 0;
            font-size: 14px;
        }

        /* Mobile View: 1-column layout */
        @media (max-width: 768px) {
            .graph-container {
                grid-template-columns: 1fr;
            }

            .graph-item {
                flex-direction: column;
            }

            .graph-img,
            .graph-desc {
                width: 100%;
                text-align: center;
            }
        }
    </style>
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3MEXW2XNBM"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-3MEXW2XNBM');
</script>

<body>
    <div class="app">
        <!-- Mobile Nav Overlay -->
        <div id="navOverlay" class="nav-overlay hidden"></div>

        <!-- Navigation -->
        <nav id="sidebar" class="sidebar">
            <div class="nav-content">
                <button id="closeNav" class="close-nav">&times;</button>
                <h2 class="nav-title">INDEX</h2>
                <div class="chapters">
                    <div class="chapter">
                        <h2><a href="Chapter1.html#chapter0">0. Links & Overview</a></h2>
                        <ul>
                            <li>Notes PDF Download Link</li>
                            <li>Github Repo Link</li>
                            <li>Project 1: Kaggle</li>
                            <li>Project 2: Kaggle</li>
                        </ul>
                    </div>
                    <div class="chapter">
                        <h2><a href="#chapter1">1. Getting started with Machine Learning</a></h2>
                        <ul>
                            <li><a href="Chapter1.html#WhatisMachineLearning">What is Machine Learning?</a></li>
                            <li><a href="Chapter1.html#HowMachineLearningWorks">How Machine Learning Works</a></li>
                            <li><a href="Chapter1.html#TypesofMachineLearning">Types of Machine Learning</a></li>
                            <li><a href="Chapter1.html#TypesofData">Types of Data</a></li>
                        </ul>
                    </div>
                    <div class="chapter">
                        <h2><a href="Chapter2.html#chapter2">2. Graphical & Analytical Representation of Data</a></h2>
                        <ul>
                            <li><a href="Chapter2.html#DataAnalysis&EDA">Data Analysis & EDA</a></li>
                            <li><a href="Chapter2.html#GraphicalRepresentationofData">Graphical Representation of
                                    data</a></li>
                            <li><a href="Chapter2.html#LimitationOfTraditionalDA">Limitation of traditional Data
                                    Analysis</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="Chapter3.html#chapter3">3. Python as a Data-Analysis Tool</a></h2>
                        <ul>
                            <li><a href="Chapter3.html#WhyPython">Why Python?</a></li>
                            <li><a href="Chapter3.html#JupyterNotebook">Jupyter Notebook</a></li>
                            <li><a href="Chapter3.html#PythonDataTypes">Data Types in Python</a></li>
                            <li><a href="Chapter3.html#BasicOperations">Basic Operations</a></li>
                            <li><a href="Chapter3.html#PythonConstructs">Condition Statements & Loops</a></li>
                            <li><a href="Chapter3.html#PythonFunctions">Functions in Python</a></li>
                            <li><a href="Chapter3.html#BasicLibraries">Basic Libraries</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="Chapter4.html#chapter4">4. Basic Data Exploration</a></h2>
                        <ul>
                            <li><a href="Chapter4.html#PandasDataframe">Pandas Dataframe</a></li>
                            <li><a href="Chapter4.html#DescriptiveStatistics">Descriptive Statistics of Data</a></li>
                            <li><a href="Chapter4.html#Numpy">Numpy: A Statistics Module in Python</a></li>
                            <li><a href="Chapter4.html#Matplotlib">Matplotlib: Graph Plotting</a></li>
                            <li><a href="Chapter4.html#Pandas">Pandas: Some important Functions</a></li>
                            <li><a href="Chapter4.html#Univariate">Univariate Analysis</a></li>
                            <li><a href="Chapter4.html#Outliers">Treating Outliers</a></li>
                            <li><a href="Chapter4.html#Correlation">Correlation</a></li>
                            <li><a href="Chapter4.html#ANOVA">ANOVA</a></li>
                            <li><a href="Chapter4.html#TrainingDatasets">Creating Training Datasets</a></li>
                            <li><a href="Chapter4.html#FeatureScaling">FeatureScaling</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="Chapter5.html#chapter5">5. Regression Modelling</a></h2>
                        <ul>
                            <li><a href="Chapter5.html#RegressionIntroduction">Introduction</a></li>
                            <li><a href="Chapter5.html#ModelEvaluationMetrics">Model Evaluation Metrics</a></li>
                            <li><a href="Chapter5.html#LinearRegression">Linear Regressin</a></li>
                            <li><a href="Chapter5.html#CostFunctionCurve">Cost Function Curve</a></li>
                            <li><a href="Chapter5.html#GradientDescent">Gradient Descent</a></li>
                            <li><a href="Chapter5.html#AssumptionsofLinearReg">Assumptions of Linear Regression</a></li>
                            <li><a href="Chapter5.html#StepsofLinearReg">Steps of Linear Regression</a></li>
                            <li><a href="Chapter5.html#OutcomeOfLinearReg">Outcome of Linear Regression</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="Chapter6.html#chapter6">6. Feature Engineering</a></h2>
                        <ul>
                            <li><a href="Chapter6.html#FetEngIntroduction">Introduction</a></li>
                            <li><a href="Chapter6.html#TransformationTech">Transformation Techniques</a></li>
                            <li><a href="Chapter6.html#CategoricalEncoding">Categorical Encoding</a></li>
                            <li><a href="Chapter6.html#FeatureExtraction">Feature Extraction</a></li>
                            <li><a href="Chapter6.html#DimensionalityReduction">Dimensionality Reduction</a></li>
                            <li><a href="Chapter6.html#AdvancedDimensionalityReduction">Advanced Dimensionality
                                    Reduction</a></li>
                            <li><a href="Chapter6.html#ForwardSelection">Forward Selection</a></li>
                            <li><a href="Chapter6.html#BackwardSelection">Backward Selection</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="Chapter7.html#chapter7">7. Logistic Regression</a></h2>
                        <ul>
                            <li><a href="Chapter7.html#LogisticIntroduction">Introduction</a></li>
                            <li><a href="Chapter7.html#LogisticEvalMetric">Evaluation Metrics</a>
                                <ul>
                                    <li><a href="Chapter7.html#ConfusionMatrix">Confusion Matrix</a></li>
                                    <li><a href="Chapter7.html#Accuracy">Accuracy</a></li>
                                    <li><a href="Chapter7.html#Precision">Precision</a></li>
                                    <li><a href="Chapter7.html#Recall">Recall</a></li>
                                    <li><a href="Chapter7.html#LogLoss">Log Loss Model</a></li>
                                    <li><a href="Chapter7.html#AucRoc">AUC ROC Curve</a></li>
                                </ul>
                            </li>
                            <li><a href="Chapter7.html#Implement">Implementation</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="Chapter8.html#chapter8">8. Decision Trees</a></h2>
                        <ul>
                            <li><a href="Chapter8.html#paravsnonpara">Parametric v/s Non-Parametric Models</a></li>
                            <li><a href="Chapter8.html#WorkingofDT">Working of Decision Trees</a></li>
                            <li><a href="Chapter8.html#TypesofDT">Types of Decision Trees</a></li>
                            <li><a href="Chapter8.html#Splitting">Splitting Criteria of Tree Nodes</a>
                                <ul>
                                    <li><a href="Chapter8.html#gini">Gini Impurity</a></li>
                                    <li><a href="Chapter8.html#IG">Information Gain</a></li>
                                    <li><a href="Chapter8.html#gini">Decision Tree Regressor</a></li>
                                </ul>
                            </li>
                            <li><a href="Chapter8.html#Implementation">Implementation</a>
                                <ul>
                                    <li><a href="Chapter8.html#Initiation">Initiation</a></li>
                                    <li><a href="Chapter8.html#Visualization">Visualization</a></li>
                                    <li><a href="Chapter8.html#Improvements">HyperParameter Tuning</a></li>
                                </ul>
                            </li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="#chapter9">9. Ensemble Models</a></h2>
                        <ul>
                            <li><a href="#Introduction">Ensemble Models</a></li>
                            <li><a href="#Bagging">Bagging: Random Forest</a></li>
                            <li><a href="#Implementation">Implementation</a></li>
                            <li><a href="#Tuning">Hyper Parameter Tuning</a></li>
                        </ul>
                    </div>

                    <div class="chapter">
                        <h2><a href="Chapter10.html#chapter10">10. Unsupervised Learning</a></h2>
                        <ul>
                            <li><a href="Chapter10.html#Introduction">Introduction</a></li>
                            <li><a href="Chapter10.html#clusters">Clusters & it's properties</a></li>
                            <li><a href="Chapter10.html#kmeans">K-Means Clustering</a></li>
                            <li><a href="Chapter10.html#implementation">Implementation</a></li>
                            
                        </ul>
                    </div> 

                    <div class="chapter" style="border: 2px solid #0d47a1; border-radius: 25px; padding: 10px 20px; display: inline-block; background: #0d47a1; color: white; text-align: center; transition: 0.3s; font-weight: bold;">
                        <h2 style="margin: 0;">
                            <a href="mlfeedback.html" style="text-decoration: none; color: white; display: block;">FEEDBACK FORM</a>
                        </h2>
                    </div>
                </div>
            </div>
        </nav>


        <!-- Main Content -->
        <div class="main-content">
            <!-- Header -->
            <header class="header">
                <button id="menuButton" class="menu-button">
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
                        stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <line x1="4" x2="20" y1="12" y2="12" />
                        <line x1="4" x2="20" y1="6" y2="6" />
                        <line x1="4" x2="20" y1="18" y2="18" />
                    </svg>
                </button>
                <button id="themeToggle" class="theme-toggle">
                    <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="4" />
                        <path d="M12 2v2" />
                        <path d="M12 20v2" />
                        <path d="m4.93 4.93 1.41 1.41" />
                        <path d="m17.66 17.66 1.41 1.41" />
                        <path d="M2 12h2" />
                        <path d="M20 12h2" />
                        <path d="m6.34 17.66-1.41 1.41" />
                        <path d="m19.07 4.93-1.41 1.41" />
                    </svg>
                    <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z" />
                    </svg>
                </button>

                <div class="right-buttons">
                    <a href="https://shivrajanand.github.io#contact" class="btn">CONNECT</a>
                    <a href="https://shivrajanand.github.io" class="btn">PORTFOLIO</a>
                </div>
            </header>

            <!-- Content -->
            <main class="content">
                <article class="article">
                    <section id="chapter9">
                        <h2 id="Introduction">Ensemble Models</h2>
                        <p>The idea behind <strong>Ensemble Modeling</strong> is that using multiple models together
                            often leads to better predictions than using a single model. The final prediction is based
                            on the combined results of individual models.</p>

                        <h3>Methods of Ensemble Modeling</h3>
                        <ul>
                            <li><strong>Mode/Vote (for Classification):</strong> In this method, we create <em>n</em>
                                models, each predicting a category. The final prediction is the <strong>mode</strong>
                                (most frequently occurring value) among them.</li>
                            <li><strong>Averaging (for Regression):</strong> For continuous variables, we take the
                                <strong>average</strong> of predictions from different models to determine the final
                                output.
                            </li>
                        </ul>

                        <h3>Pros of Ensemble Modeling</h3>
                        <ul>
                            <li>Captures diverse signals and patterns.</li>
                            <li>Reduces incorrect predictions.</li>
                            <li>Minimizes overfitting.</li>
                            <li>Improves overall model performance.</li>
                        </ul>

                        <h3>Cons of Ensemble Modeling</h3>
                        <ul>
                            <li>Increases model complexity.</li>
                            <li>Lacks interpretability, which can be a concern in business and finance.</li>
                            <li>Requires more computational resources.</li>
                        </ul>


                        <h2 id="Bagging">Bagging: Random Forest</h2>
                        <p><strong>Bootstrap Sampling</strong> is a method of creating different training datasets for
                            models by randomly sampling from the original dataset <em>with replacement</em>.</p>

                        <h3>How Bootstrap Sampling Works?</h3>
                        <ul>
                            <li>Each model receives a <strong>random sample</strong> from the original dataset.</li>
                            <li>Sampling is done <strong>with replacement</strong>, meaning the same data point can
                                appear multiple times.</li>
                            <li>For example, if we have 26 data points: <code>a, b, c, d, …, z</code>:
                                <ul>
                                    <li>We randomly pick one data point and place it in <strong>Bootstrap Sample
                                            1</strong>.</li>
                                    <li>This process is repeated <strong>26 times</strong> (allowing repetitions) until
                                        the sample is complete.</li>
                                    <li>Similarly, we can create multiple bootstrap samples for different models.</li>
                                </ul>
                            </li>
                        </ul>

                        <h3>Mathematical Interpretation</h3>
                        <p>For <strong>n</strong> data points, a total of <strong>n!</strong> bootstrap samples can be
                            created.</p>
                        <div class="random-forest-container">
                            <h2>Random Forest: A Special Case of Bagging</h2>

                            <p><strong>Random Forest</strong> is an ensemble learning method where multiple decision
                                trees are trained, and their predictions are combined to make the final decision.</p>

                            <h3>How Random Forest Works?</h3>
                            <ul>
                                <li>The base model for Random Forest is a <strong>Decision Tree</strong>.</li>
                                <li>Each tree in the forest makes an independent prediction.</li>
                                <li>The final prediction is made by <strong>combining</strong> the results of all trees
                                    (majority vote for classification, average for regression).</li>
                            </ul>

                            <h3>Randomness in Random Forest</h3>
                            <ul>
                                <li><strong>Row Sampling (Data Sampling):</strong>
                                    <ul>
                                        <li>Each tree is trained on a <strong>random subset</strong> of data (bootstrap
                                            sampling).</li>
                                    </ul>
                                </li>
                                <li><strong>Feature Sampling:</strong>
                                    <ul>
                                        <li>At <strong>each split</strong>, a random subset of features is considered
                                            instead of using all features.</li>
                                    </ul>
                                </li>
                            </ul>

                            <p>This results in a **two-fold randomness effect**:
                                <br>✔ Data randomness (at tree level)
                                <br>✔ Feature randomness (at split level)
                            </p>
                        </div>


                        <h2 id="Implementation">Implementation</h2>
                        <p>Firstly, an instance of the bagging-classifier is created under alias BC.
                            Next, we fit the training data in the classifier instance.
                        </p>
                        <div class="notebook">
                            <div class="cell">
                                <pre><code>from sklearn.ensemble import BaggingClassifier as BC <br> classifier = BC() <br> classifier.fit(X_train, y_train)</code></pre>
                            </div>
                        </div>

                        <p>base estimator is a logistic regression model.
                            n-estimators specify number of logistic regression models used to predict final results
                            n-jobs = -1 means all cores of CPU will be used.
                        </p>
                        <div class="notebook">
                            <div class="cell">
                                <pre><code>from sklearn.linear_model import LogisticRegression as LR<br><br>classifier = BC(base_estimator=LR(),<br>n_estimators=150,<br>n_jobs=-1,<br>random_state=42)<br><br>classifier.fit(X_train, y_train)<br>predicted_values = classifier.predict(X_train)<br></code></pre>
                            </div>
                        </div>


                        <p>Classification report over test and train data shows a very poor result which is unexpected
                            as bagging algorithm was supposed to be good. This is because we used a linear model as
                            base. And there is no feature transformation over the dataset and hence every logistic
                            regression model is underfit. </p>
                        <div class="notebook">
                            <div class="cell">
                                <pre><code>from sklearn.metrics import classification_report <br> print(classification_report(y_train, predicted_values))</code></pre>
                                <pre><div style="display: flex; justify-content: left; margin: 20px 0;">
                                    <table style="border-collapse: separate; border-spacing: 10px; color: var(--text-color, #ddd); background: var(--bg-color); padding: 10px;">
                                        <tr>
                                            <th> </th>
                                            <th>Precision</th>
                                            <th>Recall</th>
                                            <th>F1-Score</th>
                                            <th>Support</th>
                                        </tr>
                                        <tr>
                                            <td>0</td>
                                            <td>0.82</td>
                                            <td>0.99</td>
                                            <td>0.90</td>
                                            <td>14234</td>
                                        </tr>
                                        <tr>
                                            <td>1</td>
                                            <td>0.75</td>
                                            <td>0.08</td>
                                            <td>0.15</td>
                                            <td>3419</td>
                                        </tr>
                                        <tr>
                                            <td colspan="5"> </td>
                                        </tr>
                                        <tr>
                                            <td>Accuracy</td>
                                            <td colspan="3">0.82</td>
                                            <td>17653</td>
                                        </tr>
                                        <tr>
                                            <td>Macro Avg</td>
                                            <td>0.78</td>
                                            <td>0.54</td>
                                            <td>0.52</td>
                                            <td>17653</td>
                                        </tr>
                                        <tr>
                                            <td>Weighted Avg</td>
                                            <td>0.80</td>
                                            <td>0.82</td>
                                            <td>0.75</td>
                                            <td>17653</td>
                                        </tr>
                                    </table>
                                </div>
                                </pre>
                            </div>

                        </div>

                        <div class="notebook">
                            <div class="cell">
                                <pre><code>predicted_values = classifier.predict(X_test) <br> print(classification_report(y_test, predicted_values))</code></pre>
                                <pre><div style="display: flex; justify-content: left; margin: 20px 0;">
                                    <table style="border-collapse: separate; border-spacing: 10px; color: var(--text-color, #ddd); background: var(--bg-color); padding: 10px;">
                                        <tr>
                                            <th> </th>
                                            <th>Precision</th>
                                            <th>Recall</th>
                                            <th>F1-Score</th>
                                            <th>Support</th>
                                        </tr>
                                        <tr>
                                            <td>0</td>
                                            <td>0.82</td>
                                            <td>0.99</td>
                                            <td>0.90</td>
                                            <td>3559</td>
                                        </tr>
                                        <tr>
                                            <td>1</td>
                                            <td>0.78</td>
                                            <td>0.09</td>
                                            <td>0.16</td>
                                            <td>855</td>
                                        </tr>
                                        <tr>
                                            <td colspan="5"> </td>
                                        </tr>
                                        <tr>
                                            <td>Accuracy</td>
                                            <td colspan="3">0.82</td>
                                            <td>4414</td>
                                        </tr>
                                        <tr>
                                            <td>Macro Avg</td>
                                            <td>0.80</td>
                                            <td>0.54</td>
                                            <td>0.53</td>
                                            <td>4414</td>
                                        </tr>
                                        <tr>
                                            <td>Weighted Avg</td>
                                            <td>0.81</td>
                                            <td>0.82</td>
                                            <td>0.76</td>
                                            <td>4414</td>
                                        </tr>
                                    </table>
                                </div>
                                </pre>
                            </div>

                        </div>






                        <h2 id="Tuning">Hyper Parameter Tuning</h2>
                        <div class="graph-container">

                            <!-- Image 1 -->
                            <div class="graph-item">
                                <div class="graph-img">
                                    <img src="MLNotesIMGs/45.png" alt="Graph 1">
                                </div>
                                <div class="graph-desc">
                                    <h3>Variation of Train & Test Scores</h3>
                                    <p>The adjacent graph shows the variation of train score(Red) and test score(blue)
                                        with n-estimators over a range of 1 to 600 on steps of 10.
                                        It is clear that after some value the score difference almost becomes constant.
                                        It is not advisable to use large number of estimators because it will not affect
                                        the model performance but increase computational complexity.
                                    </p>
                                </div>
                            </div>

                            <!-- Image 2 -->
                            <div class="graph-item">
                                <div class="graph-img">
                                    <img src="MLNotesIMGs/46.png" alt="Graph 2">
                                </div>
                                <div class="graph-desc">
                                    <h3>Model Performance variation with dataset fraction</h3>
                                    <p>Performance of the model rises sharply and then saturates quickly.

                                        From this graph we can conclude that:
                                        (Amount of bootstrapped data = amount of original data) this condition is not
                                        necessary.

                                        Clearly model performance reaches to the max when data provided is less than 0.2
                                        fraction of the original dataset.

                                        Giving lesser data will significantly reduce the training time,
                                    </p>
                                </div>
                            </div>

                            <!-- Image 3 -->
                            <div class="graph-item">
                                <div class="graph-img">
                                    <img src="MLNotesIMGs/47.png" alt="Graph 3">
                                </div>
                                <div class="graph-desc">
                                    <h3>Model Performance vs Number of features</h3>
                                    <p>Performance of model initially increases.

                                        After a point train score increases whereas test score starts to saturate or
                                        decrease, which clearly means model is overfitting after some point

                                        Default value of this parameter is set to square root of number of features
                                        present in the dataset.
                                        Ideal number of max features is close to the default value.
                                    </p>
                                </div>
                            </div>

                        </div>

                    </section>
                    <section id="next">
                        <a href="Chapter10.html#Chapter10">NEXT--></a>
                    </section>


                </article>

            </main>
        </div>
    </div>

    <!-- Load scripts with defer -->
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script defer src="../../assets/js/globalblog.js"></script>


    <!-- Fallback for Prism if CDN fails -->
    <script>
        window.addEventListener('load', function () {
            if (typeof Prism === 'undefined') {
                console.error('Prism failed to load');
                // Add a class to show code without highlighting
                document.querySelectorAll('pre code').forEach(block => {
                    block.classList.add('no-highlight');
                });
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

</body>

</html>